{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybc0eOsOHVFL"
   },
   "source": [
    "# Creating tensor and manipulating them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueRF9VNEEKsv"
   },
   "source": [
    "## Creating tensors and basic properties\n",
    "\n",
    "**Create Tensors**\n",
    "* `torch.tensor()`: create random tensor with given structure and numbers\n",
    "* `torch.rand()`: create random tensor with given dimensions\n",
    "* `torch.zeros()`: create a tensor filled with zeros\n",
    "* `torch.ones()`: create a tensor filled with ones\n",
    "* `torch.arange()`: create a range (similar to function `range` but output is a tensor)\n",
    "\n",
    "**Attributes**\n",
    "* `.dtype`: data type\n",
    "* `.type()`: assign a new type\n",
    "* `.shape`: shape of the tensor\n",
    "* `.device`: on which device the tensor lives\n",
    "\n",
    "**Misc**\n",
    "* `torch.manual_seed()`: to reset the seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "up2jZKiLEhTi"
   },
   "outputs": [],
   "source": [
    "import tensor\n",
    "\n",
    "print(torch.tensor[7, 7])\n",
    "print(torch.tensor[1, 2], [3, 4]])\n",
    "print(torch.tensor[[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys3D1zm0EKou"
   },
   "source": [
    "## Manipulating tensors\n",
    "\n",
    "* `.reshape()`: change the shape of the tensor\n",
    "* `.view()`: change the view: creates a new view, the 2 tensors data are the same, changing one tensor changes the other as well but the shape of the view will be different\n",
    "* `.stack()`: stack tensors of compatible dimensions\n",
    "* `.permute()`: change order of dimensions, *useful to move colour channel first to last and viceversa*\n",
    "* `.squeeze()` and `.unsqueeze()`: remove or add dimensions to a tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_iDUIwIEKmX"
   },
   "source": [
    "# PyTorch `nn.Module()`\n",
    "\n",
    "* `nn.Module()`: class to define models: when subclassing they need a self and a forward method inside\n",
    "* `nn.Parameters()`: to manually define parameters\n",
    "* loss functions:\n",
    "  * `nn.L1Loss`: MSE loss for fitting linear models\n",
    "  * `nn.CrossEntropyLoss`: cross entropy for multi-class classification\n",
    "  * `nn.BCEWithLogitsLoss()`: for binary classification, includes sigmoid activation function. Outputs logits, use `torch.sigmoid()` to transform into predictions/probabilities\n",
    "* `torch.optim.SGD()`: Stochastic Gradient Descent algorithm\n",
    "\n",
    "**`nn.Module` possible transformations**\n",
    "* `nn.Sequential()`: to put together several transformations\n",
    "* `nn.Flatten()`: transform a multi-dimensional tensor in a vector\n",
    "* `nn.Linear()`: for linear transformation (e.g. simple linear regression)\n",
    "* `nn.Conv2d()`: convolution step\n",
    "* `nn.MaxPool2d()`: take maximum over a square of pixels and reduce dimensions\n",
    "* `nn.ReLU()`: rectified linear activation function $max(0,x)$\n",
    "* `nn.GELU()`: Gaussian Error Linear Units function\n",
    "* `nn.MultiheadAttention()`: multihead self attention block\n",
    "* `nn.Parameters()`: to create ad-hoc parameters\n",
    "\n",
    "**Methods and Attributes for a model:**\n",
    "* `a_model.state_dict()`: to get dictionary of parameters\n",
    "* `a_model.eval()`, `a_model.train()`: eval and train status\n",
    "* `with torch.inference()`: to turn off gradients, necessary when forecasting or calculating test performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4vB8PBxp9m4"
   },
   "source": [
    "## Fitting a model\n",
    "\n",
    "1. Set up number of epochs (iteration)\n",
    "1. Set up epochs loop\n",
    "1. Set up loop through batches in a DataLoader\n",
    "1. `a_model.train()`: Get model in train mode\n",
    "1. `a_model(X_data)`: Do a forward pass\n",
    "1. `loss_fn(y_pred, y_test)`: Calculate the train loss\n",
    "  * Maybe necessary to transform the output: e.g. from logit to probability\n",
    "1. `optimizer.zero_grad()`: Reset the optimizer\n",
    "1. `loss_fn.backward()`: Perform loss propagation backward\n",
    "1. `optimizer.setp()`: Perform optimizer step\n",
    "\n",
    "```python\n",
    "# set the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print(f\"Epoch: {epoch}\\n---------\")\n",
    "  ### Training\n",
    "  train_loss = 0 # cumulates loss per batch\n",
    "  # Loop through batches\n",
    "  for batch, (X, y) in enumerate(train_dataloader):\n",
    "    model_0.train()\n",
    "    # forward pass\n",
    "    y_pred = model_0(X)\n",
    "    # loss\n",
    "    loss =loss_fn(y_pred, y)\n",
    "    train_loss += loss # accumulates the train loss\n",
    "    # optimizer reset\n",
    "    optimizer.zero_grad()\n",
    "    # loss backward\n",
    "    loss.backward()\n",
    "    # optimizer step: updating model parameters once per BATCH\n",
    "    optimizer.step()\n",
    "    if batch % 400 == 0:\n",
    "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples.\")\n",
    "\n",
    "  # back to epoch loop\n",
    "  # divide loss by length dataloader\n",
    "  train_loss /= len(train_dataloader)\n",
    "\n",
    "  # testing loop\n",
    "  model_0.eval()\n",
    "  test_loss, test_acc = 0, 0\n",
    "  with torch.inference_mode():\n",
    "    for X_test, y_test in test_dataloader:\n",
    "      # forward pass\n",
    "      test_pred = model_0(X_test)\n",
    "      # loss\n",
    "      test_loss += loss_fn(test_pred, y_test)\n",
    "      # accuracy\n",
    "      test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
    "    # calculate the test loss average per batch\n",
    "    test_loss /= len(test_dataloader)\n",
    "    # accuracy average\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "  print(f\"\\nTrain loss: {train_loss:.4f} | Train acc: {test_acc:.2f}%\\nTest loss: {test_loss:.4f} | Test acc: {test_acc:.2f}%\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcA3RphHyPFK"
   },
   "source": [
    "## TinyVGG architecture model\n",
    "\n",
    "```python\n",
    "class TinyVGGArchitecture(nn.Module):\n",
    "  \"\"\"\n",
    "  Model architecture replicating TinyVGG\n",
    "  from CNN explainer website.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               input_shape: int,\n",
    "               hidden_units: int, # number of hidden units, it's not the size of each concoluted picture\n",
    "               output_shape: int):\n",
    "    super().__init__()\n",
    "    # architecure: multiple blocks\n",
    "    # convolutional blocks: multiple layers\n",
    "    self.conv_block_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_shape, # convolutional 2 dimensional\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1), # we set these values in NN\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=hidden_units, # convolutional 2 dimensional\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2,\n",
    "                     stride=2) # by default same as kernel size\n",
    "    )\n",
    "    self.conv_block_2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=hidden_units,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=hidden_units,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    # last block needs to output a classifier\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=hidden_units*7*7,\n",
    "                  out_features=output_shape)\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    x = self.conv_block_1(x)\n",
    "    # print(x.shape) # to help get the right size in the Linear layer\n",
    "    x = self.conv_block_2(x)\n",
    "    # print(x.shape)\n",
    "    x = self.classifier(x)\n",
    "    # print(x.shape)\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG models' family\n",
    "\n",
    "VGG family is a series of models with 11 to 19 layers alternating:\n",
    "* Convolutional layers\n",
    "* Max pool layers\n",
    "* End with a dense layer, with 3 linear layers.\n",
    "\n",
    "Usually between linear layers and after convolutional layers, there is an activation function (ReLU).\n",
    "\n",
    "An example is the VGG-11 model, the smallest of the family:\n",
    "\n",
    "**Conv layer -> Max pool -> Conv layer -> Max pool -> 2 Conv layers -> Max pool -> 2 Conv layers -> Max pool -> 2 Conv layers -> Max pool -> 3 Linear layers -> Soft-max**\n",
    "\n",
    "In ***PYTorch*** some examples:\n",
    "* `torch.VGG11_Weights` and `torch.vgg11`\n",
    "* `torch.VGG13_Weights` and `torch.vgg13`\n",
    "* `torch.VGG16_Weights` and `torch.vgg16`\n",
    "\n",
    "https://pytorch.org/vision/stable/models/vgg.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT) architecture\n",
    "\n",
    "References:\n",
    "* https://arxiv.org/abs/1706.03762: equations below are taken from this article\n",
    "* https://arxiv.org/abs/2010.11929\n",
    "\n",
    "Let's start with an image in $3$ colour channels size $224\\times224$ ($H\\times W$).\n",
    "\n",
    "### Embedding Patches step\n",
    "\n",
    "The image is split into non-overlapping patches of size $P$ where the image size must be divisible by $P$ (e.g. $224/16=14$). The split creates $H\\times W/P^2$ patches: if $H=W=224$ and $P=16$ we have $224\\times224/16^2 = 14^2 = 196$ patches.\n",
    "\n",
    "A linear layer is applied to the patches in order to obtain a vector of size $3\\times P^2 = 3 \\times 6^2 = 768$ corresponding to each image patch.\n",
    "\n",
    "***With code*** this can be achieved with a combination of one convolutional layer (size and step equal $P$) and a linear layer with hidden units or output features equal $3\\times P^2$.\n",
    "\n",
    "Each patch is: $X^i_pE$ where $E$ is a matrix of learnable parameters, $i=1,\\ldots,14^2$, and $p$ refers to the size of each patch.\n",
    "\n",
    "Dimensions:\n",
    "$$[B, 3, 224, 224] -> [B, 196, 768]$$\n",
    "where $B$ is the batch size.\n",
    "\n",
    "A learnable vector ($x_{class}$) size $[1, 768]$ for the class is stacked on top of the output obtaining a matrix $[197, 768]$ and then a learnable matrix $E_{pos}$ of the same size is added to it tracking the position of each patch within the image.\n",
    "\n",
    "***Final output***: $[B, 197, 768]$\n",
    "\n",
    "***Summary Equation:***\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_0 = \\left[x_{class};x^1_pE;\\cdots;x^N_pE\\right] + E_{pos}\n",
    "$$\n",
    "\n",
    "***Calculating the number of parameters***\n",
    "* For the convolutional layer we have: $16$ filters of size $16^2$, hence $16^3$\n",
    "* For the linear layer we have: $768\\times 768 + 768$\n",
    "* For the class head: $768$\n",
    "* For the position head: $197*768$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder: MSA + MLP\n",
    "\n",
    "The transformer encoder has an MSA layer (Multihead Self Attention) and an MLP (Multilayer Perceptron). Before each layer a Layernorm transformation is applied to the data.\n",
    "\n",
    "***LayerNorm***: This is a normalisation process (from PyTorch documentation of `torch.nn.LayerNorm` function):\n",
    "$$\n",
    "y_{i,j,:} = \\frac{x_{i,j,:}-E(x_{i,j,:})}{\\sqrt{Var(x_{i,j,:})+\\varepsilon}}*\\gamma_{:, :, k}+\\beta_{:, :, k}\n",
    "$$\n",
    "where $\\gamma$ and $\\beta$ are learnable parameters, and $x=X[i, j, :]$ is a vector of size $768$ from the input $[B, 197, 768]$.\n",
    "\n",
    "Layernorm has no impact on the size of the data. Output is still $[B, 197, 768]$.\n",
    "\n",
    "***MSA (Multihead Self Attention)***: The output of previous step is split into $H$ heads, for simplicity let's assume $12$. The patch dimension $768$ must be divisible by the number of heads $12$. Each head is size $[B, 197, 64]$ where $64=768/12$. Also the number of heads does not influence the number of parameters for this step.\n",
    "\n",
    "Each head $\\mathbf{z}_{h,l}$ goes through the following:\n",
    "$$\n",
    "f\\left((\\mathbf{z}_{h,l}\\cdot W_{h,q}) (\\mathbf{z}_{h,l}\\cdot W_{h,k})^T\\right) (\\mathbf{z}_{h,l}\\cdot W_{h,v}) + \\mathbf{z}_{h,l}\n",
    "$$\n",
    "where $h=1,\\ldots,H$, $l=1,\\ldots,L$ with L the number of transformer encoder layers in the model, $W_{h,q}$, $W_{h,k}$, and $W_{h,v}$ are all learnable matrices of the same size. The function $f(\\cdot)$ applies 3 transformations to the product: scale, mask (only in some models) and softmax.\n",
    "\n",
    "MSA can be coded with ad-hoc function `torch.nn.MultiheadAttention()`.\n",
    "\n",
    "The output is concatenated back to size $[B, 197, 768]$ ready to input into the next step.\n",
    "\n",
    "***MLP (Multilayer Perceptron)***: this layer consists of 2 linear layers with an activation function in between (e.g. GELU): in the first layer the hidden units are quadrupled from $768$ to $3072$ and in the second layer they are projected back to $768$.\n",
    "\n",
    "In code this is translated as a sequence: Linear -> GELU -> (Dropout ->) Linear.\n",
    "\n",
    "*Transformer Encoder* has its own functions in PyTorch `torch.nn.TransformerEncoderLayer` for 1 layer and `torch.nn.TransformerEncoder` to create a sequence of transformer encoder layers.\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html#transformer-layers\n",
    "\n",
    "***Summary Equation***\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_l'=MSA(LN(\\mathbf{z}_{l-1}))+\\mathbf{z}_{l-1}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{z}_l=MSA(LN(z_l'))+\\mathbf{z}_l'\n",
    "$$\n",
    "\n",
    "\n",
    "***Calculating the number of parameters***\n",
    "* For the layernorm we have $768*2$ (this happens twice before MSA and MLP)\n",
    "* For the MSA: $12$ heads, $3$ matrices of size $768*768/12$ plut bias $3*768/12$ and a matrix $768*768$ for a fully connected layer plut the fully connected bias $768$: $3*768*768+3*768+768*768+768$\n",
    "* For the MLP: $768*3072+3072$ for the first layer and $3072*768+768$ for the second linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Head\n",
    "\n",
    "The last layer contains the classifier, projecting the outout of the last transformer encoder layer into the number of classes. It takes as input the first row of elements of $z_L^0$ and it applies a layernorm and a linear layer:\n",
    "$$\n",
    "\\mathbf{y}=LN\\left(\\mathbf{z}^0_L\\right)\n",
    "$$\n",
    "\n",
    "***Calculating number of parameters***\n",
    "\n",
    "* For the layernorm it's $768*2$ parameters\n",
    "* For the linear layer the parameters are $768*C+C$, where $C$ is the number of classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "`PyTorch` offers a wide range of model architectures with pre-trained weights. It is possible to use these models, loading pre-trained weights and adapt to the problem we are trying to solve.\n",
    "\n",
    "### Load a model\n",
    "\n",
    "There are 3 important steps in preparing an instance of a pre-trained model:\n",
    "1. Load the weights\n",
    "2. Extract the appropriate transformation: it is important that our data is transformed in the same way as the images used to train the model\n",
    "3. Create an instance of the model and load the pre-trained weights\n",
    "\n",
    "```python\n",
    "# 1. weights\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\n",
    "# 2. transform\n",
    "model_transforms = weights.transforms()\n",
    "# 3. model\n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "```\n",
    "\n",
    "### Fitting a pre-trained model\n",
    "\n",
    "The following steps:\n",
    "1. Create DataLoaders with the appropriate transform\n",
    "2. Replace the classifier to adapt to the right number of classes (basically change the very last linear layer)\n",
    "3. Freeze the gradient of all parameters in the feature extraction layers of the model so they do not get updated when fitting the model\n",
    "\n",
    "```python\n",
    "# 1. dataloaders\n",
    "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
    "                                  transform=model_transforms) # from the pre-trained weights\n",
    "# 2. freeze parameters\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "# 3. replace classifier\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True), \n",
    "    torch.nn.Linear(in_features=1280, \n",
    "                    out_features=output_shape, # same number of output units as our number of classes\n",
    "                    bias=True)).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqxWthmwtkX7"
   },
   "source": [
    "# Loading and creating datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unj4uTJPtoNY"
   },
   "source": [
    "## DataLoader\n",
    "\n",
    "`from torch.utils.data import DataLoader` to create batches of data as it's computationally impossible to use all images at the same time. Good batch size are powers of 2, like 32 or 64.\n",
    "* use `next(iter(aDataLoader))` to access one batch of data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvKja0BNy83Y"
   },
   "source": [
    "# Evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxvn_2TGtFvp"
   },
   "source": [
    "## Torchvision\n",
    "\n",
    "* `import datasets`: contains datasets\n",
    "* `import transform`: contains transformation to adapt images to correct format/size or to augment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchinfo \n",
    "\n",
    "* `from torchinfo import summary`: nice summary of model\n",
    "\n",
    "```python\n",
    "summary(model=a_model,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n",
    "```\n",
    "```\n",
    "========================================================================================================================\n",
    "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
    "========================================================================================================================\n",
    "PatchEmbedding (PatchEmbedding)          [32, 3, 224, 224]    [32, 196, 768]       --                   True\n",
    "├─Conv2d (patcher)                       [32, 3, 224, 224]    [32, 768, 14, 14]    590,592              True\n",
    "├─Flatten (flatten)                      [32, 768, 14, 14]    [32, 768, 196]       --                   --\n",
    "========================================================================================================================\n",
    "Total params: 590,592\n",
    "Trainable params: 590,592\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (G): 3.70\n",
    "========================================================================================================================\n",
    "Input size (MB): 19.27\n",
    "Forward/backward pass size (MB): 38.54\n",
    "Params size (MB): 2.36\n",
    "Estimated Total Size (MB): 60.17\n",
    "========================================================================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7IIIgVfuXXD"
   },
   "source": [
    "## Torchmetrics\n",
    "\n",
    "```python\n",
    "try:\n",
    "  import torchmetrics\n",
    "except:\n",
    "  !pip install -q torchmetrics\n",
    "  import torchmetrics\n",
    "```\n",
    "\n",
    "Contains functions to help evaluate models\n",
    "* `Accuracy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvh3aBVDwcgF"
   },
   "source": [
    "### Confusion matrix\n",
    "\n",
    "```python\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvkuJv_srVRw"
   },
   "source": [
    "# `sklearn` useful functions\n",
    "\n",
    "* `from sklearn.datasets import make_circles, moons, make_blobs`: to create artifical datasets\n",
    "* `from sklearn.model_selection import train_test_split`: to split dataset into train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNHZbqcYuseI"
   },
   "source": [
    "# Misc\n",
    "\n",
    "* `from tqdm,auto import tqdm`: to have a progress bar when running a loop\n",
    "* `from timeit import default_timer as timer`: to get system time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymBfdcpZrU9A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
